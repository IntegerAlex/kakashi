name: Performance Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'kakashi/**'
      - 'performance_tests/**'
      - '.github/workflows/performance-tests.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'kakashi/**'
      - 'performance_tests/**'
      - '.github/workflows/performance-tests.yml'
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      python_version:
        description: 'Python version to test'
        required: false
        default: '3.11'
        type: choice
        options:
          - '3.9'
          - '3.10'
          - '3.11'
          - '3.12'
      run_extended_tests:
        description: 'Run extended stability tests'
        required: false
        default: false
        type: boolean

env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1

jobs:
  performance-test:
    name: Performance Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
        include:
          - python-version: '3.9'
            python-version-short: '39'
          - python-version: '3.10'
            python-version-short: '310'
          - python-version: '3.11'
            python-version-short: '311'
          - python-version: '3.12'
            python-version-short: '312'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('performance_tests/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential python3-dev

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r performance_tests/requirements.txt

    - name: Build and install Kakashi
      run: |
        echo "üî® Building Kakashi..."
        pip install -e .
        echo "‚úÖ Kakashi installed successfully"

    - name: Run API Compatibility Tests
      run: |
        echo "üîß Running API Compatibility Tests..."
        cd performance_tests
        python -m pytest test_api_compatibility.py -v --tb=short --junitxml=api-tests-${{ matrix.python-version-short }}.xml

    - name: Run Performance Benchmark Tests
      run: |
        echo "‚ö° Running Performance Benchmark Tests..."
        cd performance_tests
        python -m pytest test_performance.py -v --benchmark-only --benchmark-sort=mean --junitxml=performance-tests-${{ matrix.python-version-short }}.xml

    - name: Run Stability Tests
      run: |
        echo "üõ°Ô∏è Running Stability Tests..."
        cd performance_tests
        python -m pytest test_stability.py -v --tb=short --junitxml=stability-tests-${{ matrix.python-version-short }}.xml

    - name: Run Extended Tests (if requested)
      if: ${{ github.event.inputs.run_extended_tests == 'true' || github.event_name == 'schedule' }}
      run: |
        echo "üî¨ Running Extended Tests..."
        cd performance_tests
        python -m pytest -v --benchmark-only --benchmark-sort=mean --junitxml=extended-tests-${{ matrix.python-version-short }}.xml

    - name: Generate Test Report
      run: |
        echo "üìä Generating Test Report..."
        cd performance_tests
        python run_tests.py --save-report --report-file=test-report-${{ matrix.python-version-short }}.json

    - name: Upload Test Results
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.python-version-short }}
        path: |
          performance_tests/*.xml
          performance_tests/test-report-*.json
          performance_tests/.pytest_cache/

    - name: Upload Benchmark Results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.python-version-short }}
        path: |
          performance_tests/.benchmarks/

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: performance-test
    if: always()
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: test-results

    - name: Generate Summary Report
      run: |
        echo "üìã Generating Summary Report..."
        python performance_tests/generate_summary.py test-results/

    - name: Upload Summary Report
      uses: actions/upload-artifact@v3
      with:
        name: summary-report
        path: |
          summary-report.md
          summary-report.json

    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summaryPath = 'summary-report.md';
          
          if (fs.existsSync(summaryPath)) {
            const summary = fs.readFileSync(summaryPath, 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## üöÄ Performance Test Results\n\n${summary}`
            });
          }